\chapter{工作笔记}
\section{转写数据训练}
\subsection{准备数据}
首先对数据进行清洗，期间碰到的坑数不胜数，防不胜防。

kaldi训练的时候，在数据准备阶段，需准备以下这些文件：
\begin{itemize}
	\item wav.scp: 格式为 \textcolor{gray}{"<index> <wave\_path>"}，存储着音频的索引和绝对路径；
	\item segments: 格式为\textcolor{gray}{"<subindex> <index> <start\_time> <end\_time>"}，存储着音频所对应的segments的子索引，原始音频索引和起止时间点（秒）。
	\item text: 格式为\textcolor{gray}{"<subindex> <word1> <word2> ..."}，存储着segment的索引和对应的文本；
	\item utt2spk: 格式为\textcolor{gray}{"<subindex> <spk>"}，存储着segment的索引及其所对应的说话人；
	\item spk2utt: 格式为\textcolor{gray}{"spk <subindex1> <subindex2> ..."}。
\end{itemize}

说明一下，一个原始音频中，不一定每个时刻都有人说话，所以我们可以得到有说话人说话部分的segments文件，这样kaldi在进行特征提取、对齐和chain model的训练的时候就只考虑那些segments了，省时省力省空间。如果没有segments文件，上述的"<subindex>"和"<index>"就是一样的了。如果有segments，那么相当于说用这些音频片段去进行对齐和训练，所以每个片段对应的文本和说话人都得有，所以text、utt2spk和spk2utt这些文件里面都是用的"<subindex>"。

在清洗数据的时候需要注意以下几点：
\begin{itemize}
	\item 音频格式要统一，我们需要的是8k，16-bit Signed Integer PCM的wav文件，所以通过sox或者FFmpeg都得进行转换；
	\item 注意待处理的文本格式，首先文本的编码方式可能不一样，其次不同的库文本标注的格式也不一样；
	\item 音频和文本不统一；
\end{itemize}

我们遇到的音频格式主要有：
\begin{itemize}
	\item WAV Sample Encoding: A-law
	\item WAV Sample Encoding: GSM
	\item WAV Sample Encoding: 16-bit Signed Integer PCM
	\item MP3
	\item PCM
	\item V3
\end{itemize}

那进行音频的格式统一之后，我们就可以得到 wav.scp，再对文本进行去大小写，去标点符号，去乱七八糟的符号之后，我们就可以得到text、segments、spk2utt和utt2spk了。在得到所有的上述文件之后，调用kaldi的脚本 utils/fix\_data\_dir.sh 来进行排序和去除无效的数据。

在处理的过程中，有一些需要注意的事情记录如下：
\begin{itemize}
	\item 在进行音频和文本匹配的时候，一定要保证音频名和文本名是匹配的，也就是说，如果找 test.txt 对应的音频，那么一定要保证该音频的名字是 test.wav；
	\item 转写文本中某一个segment标注了"有效"或者"valid"，并不一定是真的有效，还得再看看那个segment对应的文本是不是空的，在进行文本清洗的时候，要判断下文本的长度是不是等于0；
	\item 音频的格式虽然是wav，但是它不一定是kaldi能处理的wav……比如上面提到的A-law和GSM，kalid就无法处理，而A-law用python的标准库 wave去读也读不了，所以A-law很好判断，但是GSM wave可以读……所以一定要保证音频格式的统一；
	\item 音频可能是双通道的；
	\item 音频可能头文件存储的音频长和实际长度不一样；
	\item 音频可能是损坏的……即全是噪声，频谱图趋于均匀分布，这种的可以考虑算下音频的RMS，如果RMS比正常音频大很多，那说明就是有问题的，因为噪声越大，RMS越大；
	\item 有的音频虽然以wav为后缀名，实际上人家是pcm；
	\item 限制变量的正负，比如segment的起止点，肯定是大于等于0的。
\end{itemize}

\subsection{准备词典和词表}
我们准备好了语音和文本，那如何将二者联系起来呢？简单来说，如图\ref{fig:basic-theory}，HMM连接了音频特征和音素序列，而音频特征由原始音频提取得到，音素序列可以通过词典转换得到，因此词典是十分重要的。
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{chap1-1}
  \caption{原始音频和文本是如何连接起来的 \label{fig:basic-theory}}
\end{figure}

那么在准备词典之前，我们首先要知道整个数据集中有哪些词，所以首先我们要对所有的文本进行分词，在上面我们提到的 text 文件中，其实就已经是按照词的方式呈现出来了。比如说有句话的索引为A00001，其对应的文本是"我在吃饭"，那么text文件中存储的格式就是：
\begin{lstlisting}[language=shell, numbers=left, 
         numberstyle=\tiny,keywordstyle=\color{blue!70},
         commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox,
         rulesepcolor=\color{red!20!green!20!blue!20},basicstyle=\ttfamily]
A00001 我 在 吃饭
\end{lstlisting}

词典的格式为 \textcolor{gray}{"<word> <pronouciations>"}。

除却发音词典，词表（vocabulary）用于语言模型的训练，一般是N-gram。所以同样，我们还需要一个vocab.txt文件。其每一行都有一个word。

\subsection{开始训练}



\subsection{第一轮训练结果记录与分析}
经过第一轮训练之后，对提取的测试集进行解码，为了看不同语言模型下模型的性能，我们采用了四种语言模型去进行解码，这四种LM的特点如下：
\begin{itemize}
	\item 整个数据集包含的文本训练的LM，记作all-lm；
	\item 大语言模型测试的小语言模型，记作small-lm；
	\item 大语言模型进行rescore，记作big-rescore-lm。
\end{itemize}

经过这些不同的语言模型解码之后得到的CER结果如表\ref{tab:lm-decoder}。
\begin{table}[h]
 \centering
 \caption{不同语言模型解码的测试集CER}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccc}
	 \toprule
		{\bf 语言模型} & {\bf 大小} & {\bf CER(\%)} \\
	 \midrule
	   all-lm         &  83M   & 22.97    \\
	   small-lm       &  50M   & 28.45    \\
	   big-rescore-lm &  3.7G  & 27.58    \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:lm-decoder}%
\end{table}%


% \begin{table}[h]
%  \centering
%  \caption{不同语言模型解码的测试集CER}
% 	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccccccc}
% 	 \toprule
% 		{\bf 语言模型} & {\bf 大小} & {\bf CE(\%)} &{\bf INS} &{\bf DEL}  &{\bf SUB} &{\bf Total}\\
% 	 \midrule
% 	   all-lm         &  83M   & 22.97   &   7420 &   22157 & 29619  & 257710 \\
% 	   small-lm       &  50M   & 28.45   &   7376 &   23291 & 42650  & 257710 \\
% 	   big-rescore-lm &  3.7G  & 27.58   &   7169 &   23825 & 40092  & 257710 \\
% 	 \bottomrule
% 	 \end{tabular*}%
%  \label{tab:lm-decoder}%
% \end{table}%

转写数据实际上由12个数据库组成，对每一个数据库单独统计CER，得到表\ref{tab:cer-per}。
\begin{table}[h]
 \centering
 \caption{测试集各个库的CER}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccc}
	 \toprule
		{\bf Project   } & {\bf ID   } & {\bf CER(\%)  } \\
	 \midrule
		TR2017097  &         005 &        21.50 \\
		TR2018011  &         011 &        17.49 \\
		TR2018035  &         013 &        24.47 \\
		\textcolor{red}{TR2018046}  &         \textcolor{red}{016} &         \textcolor{red}{79.79} \\
		TR2018063  &         018 &        23.44 \\
		TR2018067  &         020 &        28.91 \\
		TR2018077  &         022 &        15.27 \\
		TR2018086  &         025 &        35.15 \\
		TR2018092  &         026 &        36.16 \\
		TR2018109  &         030 &        25.98 \\
		TR2019068  &         044 &        24.32 \\
		kefu       &         049 &        13.41 \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:cer-per}%
\end{table}%
% \begin{table}[h]
%  \centering
%  \caption{不同语言模型解码的测试集CER}
% 	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccccccc}
% 	 \toprule
% 		{\bf Project   } & {\bf ID   } & {\bf #WRDS   } & {\bf #SUB    } & {\bf #INS    } & {\bf #DEL    } & {\bf CER  } \\
% 	 \midrule
% 		TR2017097  &          5 &      15480 &       2416 &        361 &        552 &   0.215052 \\
% 		TR2018011  &         11 &       5511 &        716 &         87 &        161 &   0.174923 \\
% 		TR2018035  &         13 &      39569 &       7209 &       1038 &       1439 &   0.244788 \\
% 		\textcolor{red}{TR2018046}  &         \textcolor{red}{16} &      \textcolor{red}{20610} &       \textcolor{red}{3725} &        \textcolor{red}{518} &      \textcolor{red}{12202} &   \textcolor{red}{0.797914} \\
% 		TR2018063  &         18 &       5821 &       1021 &        119 &        225 &   0.234496 \\
% 		TR2018067  &         20 &       6959 &       1231 &        419 &        362 &   0.289122 \\
% 		TR2018077  &         22 &      11382 &       1078 &        272 &        389 &   0.152785 \\
% 		TR2018086  &         25 &       2677 &        648 &         91 &        202 &   0.351513 \\
% 		TR2018092  &         26 &      28489 &       6890 &       1127 &       2285 &   0.361613 \\
% 		TR2018109  &         30 &      37833 &       6641 &        744 &       2447 &   0.259879 \\
% 		TR2019068  &         44 &      17187 &       3089 &        344 &        748 &   0.243265 \\
% 		kefu       &         49 &      66192 &       5270 &       1789 &       1821 &   0.134155 \\
% 	 \bottomrule
% 	 \end{tabular*}%
%  \label{tab:cer-per}%
% \end{table}%

\subsection{无效数据剔除}
从表\ref{tab:cer-per}中可以看出，数据库TR2018046是有问题的，我们通过计算该库中每一条语句的WER，排序之后发现很多的语句的CER比1大，或者接近于1，说明整个句子就基本就是没有识别对的。该库的单句CER示例如表\ref{tab:tr2018046}。
\begin{table}[h]
 \centering
 \caption{TR208046库逐句CER示例}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}cccccc}
	 \toprule
		{\bf Utt\_ID  }  & {\bf #WRDS   } & {\bf #SUB    } & {\bf #INS    } & {\bf #DEL    } & {\bf CER} \\
	 \midrule
		016000000002521    & 	51     & 	27    &  	37    &  	6     &  	1.3725 \\
		016000000004530    & 	127    & 	90    &  	55    &  	18    &  	1.2835 \\
		016000000003068    & 	95     & 	55    &  	58    &  	8     &  	1.2737 \\
		016000000003386    & 	88     & 	48    &  	55    &  	8     &  	1.2614 \\
		016000000010258    & 	34     & 	19    &  	23    &  	0     &  	1.2353 \\
		016000000001269    & 	68     & 	21    &  	26    &  	33    &  	1.1765 \\
		016000000002156    & 	42     & 	17    &  	27    &  	5     &  	1.1667 \\
		016000000006262    & 	152    & 	66    &  	44    &  	64    &  	1.1447 \\
		016000000001991    & 	144    & 	48    &  	30    &  	84    &  	1.125  \\
		016000000008699    & 	101    & 	51    &  	25    &  	36    &  	1.1089 \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:tr2018046}%
\end{table}%

但是库TR208046仍然有部分数据是正常的，所以我们需要将异常的数据剔除掉。以音频 R87\_A1629\_0015054069879\_5aa8cf1d.wav为例，其频谱图如\ref{fig:audio-freq}（左），而未损坏的音频的频谱图应该类似于图\ref{fig:audio-freq}（右）。
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.40\textwidth]{figure/error-audio}
	\hspace{1cm}
	\includegraphics[width=0.40\textwidth]{figure/right-audio}
	\caption{损坏音频（左）与未损坏音频（右）频谱图}
\label{fig:audio-freq}
\end{figure}

损坏的音频听起来完全就是噪音，从频谱图上看整个频域信息有点类似于均匀分布，我们通过计算音频每一帧的RMS（Root-Mean-Square），结合每条音频的CER，交叉验证音频是否有效。RMS的计算如公式\ref{eqn:rms}。
\begin{align}
\label{eqn:rms}
	x_{\mathrm{RMS}}=\sqrt{\frac{1}{n}\left(x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}\right)}
\end{align}

根据python处理音频的librosa库中所建议的，计算音频频谱值的RMS更为准确，所以再计算了音频每一帧的频谱值RMS之后，求整个音频的均值，通过观察损坏音频和未损坏音频的RMS均值，我们认定RMS均值6000以上且该句CER超过60\%的音频为无效音频，将其剔除。

剔除之后，我们进行了第二轮训练。

\subsection{第二轮训练过程及结果}
去除掉训练集和测试集中的无效数据之后，我们进行了第二轮的训练，训练的结果如表\ref{tab:second-train}所示。
\begin{table}[h]
 \centering
 \caption{2nd训练后不同LM下的测试结果}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}cccc}
	 \toprule
		{\bf 语言模型} & {\bf 大小} & {\bf CER(\%)} &{\bf VAR} \\
	 \midrule
	   all-lm         &  83M   & 17.98   &  -4.99 \\
	   small-lm       &  50M   & 24.04   &  -4.41 \\
	   big-rescore-lm &  3.7G  & 22.97   &  -4.61 \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:second-train}%
\end{table}%

而这个时候每个库的CER及其与第一轮训练的差值如表\ref{tab:second-each}。
\begin{table}[h]
 \centering
 \caption{2nd训练后不同库的CER}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}cccc}
	 \toprule
		{\bf Project   } & {\bf ID   }  & {\bf CER(\%)   } & {\bf Var(\%) }\\
	 \midrule
		TR2017097 &	005  &  15.49   &  -0.51  \\
		TR2018011 &	011  &  10.05   &   0.18   \\
		TR2018035 &	013  &  18.24   &   0.12   \\
		TR2018046 &	016  &  16.89   &  -61.31    \\
		TR2018063 &	018  &  14.06   &   0.24   \\
		TR2018067 &	020  &  26.45   &   0.12   \\
		TR2018077 &	022  &  10.85   &  -0.10   \\
		TR2018086 &	025  &  27.53   &   0.03   \\
		TR2018092 &	026  &  27.57   &  -1.61   \\
		TR2018109 &	030  &  20.91   &   0.05    \\
		TR2019068 &	044  &  18.61   &   0.15   \\
		kefu      &	049  &  13.48   &   0.07   \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:second-each}%
\end{table}%

那其实除了那个有问题的库CER变化很大，剩下的库基本上也没啥变化……而整体CER减小都是库TR2018046的变化造成的。

\subsection{无关数据库测试}
为了验证最终训练出来的模型效果，我们选取了一些同类型的（客服或者电话）数据库进行识别测试。我们记录了不同数据库的识别结果，以及识别的时长。如表\ref{tab:other-corpus}。
\begin{table}[h]
 \centering
 \caption{2nd训练后模型之不相关数据测试}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}cccccc}
	 \toprule
		{\bf 数据库   } & {\bf 总时长   } & {\bf 识别时长   } & {\bf 未识别时长    } & {\bf CER    } & {\bf 类别    } \\
	 \midrule
		TR2018041 &     83.11  &    81.66  &     1.45&       32.58&       客服\/汽车  \\
		TR2017115 &     270.31 &    270.31 &     0   &       35.99&       客服        \\
		TR2017127 &     67.43  &    67.43  &     0   &       22.71&       客服        \\
		TR2018012 &     112.69 &    112.69 &     0   &       24.04&       电话数据     \\
		TR2018056 &     135.44 &    135.44 &     0   &       43.69&       电话数据\/分享报告 \\
		TR2018097 &     56.97  &    56.97  &     0   &       20.35&       客服/聊天   \\
		TR2018119 &     345.24 &    345.24 &     0   &       19.22&       电话数据     \\
		Total     &     1071.19&    ‬1069.74&‬     1.45&       27.87&      			  \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:other-corpus}%
\end{table}%

对识别效果较差的数据库进行逐句分析，单句CER接近于1的音频识别差的原因有以下几点：
\begin{itemize}
	\item 音频为歌曲，文本为转写的歌词，如TR2017115中的部分数据；
	\item 音频在会场的环境录制，混响十分严重；
	\item 音频与文本不对应，即音频的参考文本是错的；
	\item 音频无对应的参考文本；
	\item 部分音频损坏，头文件包含的音频长度与实际音频长度不一致，导致参考文本中多出来很多无对应音频的segments；
	\item 部分方言很重的音频；
\end{itemize}

另外表格中未识别时长指的是模型未能识别的音频总时长，对这些音频进行分析，大部分都是问题音频，音频的长度接近于100或者200ms，参考的文本却有好多个字，这类音频因为太短，所以模型无法识别，故而跳过了。比如说下面这个音频片段，长度为63ms，对应的文本有30多个字。测试的这些库都存在这样的情况，表格中0是因为数量太少，所以直接就忽略了。
\begin{lstlisting}[language=shell, numbers=left, 
         numberstyle=\tiny,keywordstyle=\color{blue!70},
         commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox,
         rulesepcolor=\color{red!20!green!20!blue!20},basicstyle=\ttfamily]
Index: 0170000000008-c2--1752605--1752668  
Time Length: 0.06299999999987449 
Text: 然后这位同事呢 他是做 的 当然知道什么样的东西不能做 但是经不住朋友的要求
\end{lstlisting}

\subsection{新语言模型解码结果}
新的语言模型大小为7.9 G，其发音词典有161460个词及其发音，词表有152946个词。其对应的小语言模型大小为165M，最终构图生成的G.carpa为9.3 G。目前对原有测试集以及四个无关库进行了解码测试结果如表\ref{tab:new-lm}：
\begin{table}[h]
 \centering
 \caption{新的大语言模型与原有大语言模型结果比较}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}cccc}
	 \toprule
		{\bf 数据库   } & {\bf LM   } &  {\bf CER(\%)    } & {\bf 与原有大语言模型的比较} \\
	 \midrule
		 原有测试集 & small & 22.82   & -4.61  \\
		 原有测试集 & large & 21.14   & -1.83  \\
		 TR2018056 & large & 45.11   &        \\
		 TR2018056 & small & 46.27   &        \\
		 TR2018119 & large & 17.28   &        \\
		 TR2018119 & small & 18.10   &        \\
		 TR2017115 & large & 36.69   &        \\
		 TR2017115 & small & 38.87   &        \\
		 TR2017127 & large & 26.32   &        \\
		 TR2017127 & small & 28.43   &        \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:new-lm}%
\end{table}%

比较数据集文本训练出来的语言模型与新的大语言模型结果，见表\ref{tab:new-lm1}我们发现……数据集训练出来的语言模型效果稍微好一些。
\begin{table}[h]
 \centering
 \caption{新的大语言模型与数据集文本训练的语言模型结果比较}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccc}
	 \toprule
		{\bf 数据库   }  &  {\bf CER(\%)    } & {\bf 与原始数据集文本训练处的语言模型比较} \\
	 \midrule
		 原有测试集 & 21.14   & 3.16 \\
		 TR2018056 & 45.11   & 1.42  \\
		 TR2018119 & 17.28   & -1.94 \\
		 TR2017115 & 36.69   & 0.7   \\
		 TR2017127 & 26.32   & 3.61  \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:new-lm}%
\end{table}%

\subsection{离线识别并输出timestamp}
选取了10000句短音频测试对齐效果，10000句的时长为17h，解码耗时10h，RTF=0.588（同样的数据和机器，另外一次解码耗时是6.75h，RTF=0.386）。后续所提到的时间未带单位的均为秒。解码输出的音素对齐如下所示：
\begin{lstlisting}[language=shell, numbers=left, 
         numberstyle=\tiny,keywordstyle=\color{blue!70},
         commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox,
         rulesepcolor=\color{red!20!green!20!blue!20},basicstyle=\ttfamily]
[('SIL', 0, 6), ('du_B', 6, 2), ('ehH_I', 8, 2), ('iL_E', 10, 1), ('wu_B', 11, 1), ('oL_I', 12, 1), ('oL_I', 13, 1), ('m_I', 14, 1), ('elM_I', 15, 3), ('nnM_E', 18, 1), ('m_B', 19, 1), ('ehL_I', 20, 2), ('iH_I', 22, 2), ('yi_I', 24, 1), ('oL_I', 25, 1), ('uL_E', 26, 2), ('n_B', 28, 1), ('aH_I', 29, 2), ('aL_I', 31, 1), ('g_I', 32, 4), ('elM_I', 36, 1), ('elM_E', 37, 1), ('SIL', 38, 2), ('ji_B', 40, 2), ('iH_I', 42, 2), ('iH_I', 44, 1), ('ji_I', 45, 3), ('iH_I', 48, 2), ('nnH_E', 50, 4), ('SIL', 54, 9)]
\end{lstlisting}

其中每一个tuple代表的是音素、起始帧和该音素的帧长。kaldi中是基于堆叠多帧为一个大帧，然后跳帧输入的策略\upcite{subframe}，stack 8帧，然后跳3帧，这样就不需要逐帧解码了，如图\ref{fig:subsample}而解码输出的时间是跳帧后的时间点，我们需要将其换算成对应的实际音频时间。
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{subsample}
	\caption{stacking and subsampling of frames}
\label{fig:subsample}
\end{figure}

一帧所占时间长度为25ms，提取特征的时候帧移位10ms，一次stack的帧数为8帧，跳帧为3帧，设对齐音素的起始帧为$x$，帧数为$n$，那么我们可以通过公式\ref{eqn:compute-ali}得到该音素实际的起始时间$t_{s}$和结束时间$t_{e}$：
\begin{align}
\begin{split}
\label{eqn:compute-ali}
	t_{s} &= x*30 \\
	t_{e} &= (x+y)*30 \\
\end{split}
\end{align}

得到解码出的每个音素的timestamp之后，我们将其起止时间和标注的起止时间相减，得到每一个音频解码的对齐与实际标注的差异。10000句话的解码时间节点与标注的时间节点的起止时间差如图\ref{fig:ali-sta}和\ref{fig:ali-end}所示。
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{ali-timestamp-start}
	\caption{解码和标注时间节点的起点时间差}
\label{fig:ali-sta}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{ali-timestamp-end}
	\caption{解码和标注时间节点的终点时间差}
\label{fig:ali-end}
\end{figure}

从图中，有些音频的时间差超过1s，对于超过了1s的音频，其数量为106条，占比1.06\%，之所以会出现这么大误差，有以下几种原因：
\begin{itemize}
	\item 标注的不正确，包括时间标注成负值和起止点位置不准确；
	\item 音频中标注为无有效音频的区域，模型也识别出了结果；
	\item 音频中标注为有有消音片的区域，模型未识别出结果；
\end{itemize}

我们去除掉这部分异常数据，对剩下的数据进行了分析，这些数据的基本信息如下表\ref{tab:basic-info}
\begin{table}[h]
 \centering
 \caption{离线输出timestamp的数据基本信息}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccccc}
	 \toprule
		{\bf Position} & {\bf 最小值} &{\bf 最大值} &{\bf 均值}  &{\bf 中位数} \\
	 \midrule
	   起点    &    0  &   0.999   &  \textcolor{red}{0.071487}   &   0.036   \\
	   终点    &    0  &   0.985   &  \textcolor{red}{0.049984}   &   0.027   \\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:basic-info}%
\end{table}%
   
而这些数据在各个区间的分布如表\ref{tab:offline-dis}和图\ref{fig:hist-dis}。从这些数据中我们可以得到的结论有：
\begin{enumerate}
	\item 选取的音频有10000条，去除明显无效数据，有9894条，应该可以用于说明模型用于输出对齐的效果；
	\item 数据的差异在500ms以上的，应该都是识别出来的比校对的多或者校对的比识别出来的多，类似于，这里面我们训练的模型从0.67s就开始有输出了，但是实际人工转写标注的就没有“\textcolor{gray}{你 给 我 打 了 很 多 个 地方}”这个文本内容，所以这部分数据原则上也算是无效数据，因为无法比对出实际对齐效果；
	\begin{lstlisting}[language=shell, numbers=left, 
         numberstyle=\tiny,keywordstyle=\color{blue!70},
         commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox,
         rulesepcolor=\color{red!20!green!20!blue!20},basicstyle=\ttfamily]
049000000000115 2.083 15.025 0.67 14.92
HYP:  你 给 我 打 了 很 多 个 地方 都 办 了 两 个 多 星期 了 呀 还 没有 审核 出来 这 是 嗯 那 这样 女士 这 边 看 到 您 的 是 呃 十一月 五 号 申请 的 今天 是 十八 号 也就是说 十三 天 左右 您 这边 耐心 等待 一下 后续 的 话 咱们
REF:  都半两个多星期了呀 还没有审核出来这事 嗯那单女士这边看您在是啊十一月的五号申请的 今天是十八号 也就是说十三天左右 您这两天耐心等待一下后续的话咱们
	\end{lstlisting}
	\item 根据起点和终点的时间差区间分布，大部分数据都处于$0\sim100$ms，同样根据其均值，差异均在100ms以内，这个误差我觉得应该是可以接受的；
	\item 我们可以看到一般起点的误差都会比较大，可能的原因有：（1）转写和识别开始的位置不一样，（2）识别出现错误，（3）转写有问题；
\end{enumerate}

根据以上分析，识别输出的timestamp是可以接受的，下一步是结合VAD工具，首先对长音频进行切音，然后再送给模型进行识别，这中间包含以下问题：

（1）以何种方式进行切音和识别的配合？

在调用微软的接口的时候，都是先进行切音，从utrans上导出切音结果，然后对音频进行切音，挨个将音频发送给微软的识别接口，得到返回值；但是使用kaldi训练好的模型，我们需要的只是segment文件，即包含了切片音频，原始音频，起始时间点的一个文件即可，所以VAD切音的结果和和segment格式需统一；

（2）识别返回值：包括文本和模型对齐的timestamp，再将输出的timestamp和切音的时间点进行相加，得到原始音频的时间点信息；

\begin{table}[h]
 \centering
 \caption{离线输出timestamp的误差分布}
	 \begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}ccccccccc}
	 \toprule
		{\bf 区间} &(0.0, 0.1] &(0.1, 0.15]&(0.15, 0.2]&(0.2, 0.3] &(0.3, 0.4]&(0.4, 0.5]&(0.5, 0.6] &(0.6, 1.0] \\
	 \midrule
	 	起点 & \textcolor{red}{0.676299}& 0.107394& 0.161207& 0.036149 & 0.006200& 0.002574& 0.002691& 0.007487\\
	 	终点 & \textcolor{red}{0.905645}& 0.033727& 0.020175& 0.019360 & 0.007336& 0.006521& 0.003872& 0.003363\\
	 \bottomrule
	 \end{tabular*}%
 \label{tab:offline-dis}%
\end{table}%

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.45\textwidth]{figure/dis_start}
	\hspace{0cm}
	\includegraphics[width=0.45\textwidth]{figure/dis_end}
	\caption{起点（左）与终点（右）标注时间和识别时间的差异}
\label{fig:hist-dis}
\end{figure}

\subsection{后续}
数据的清洗还得继续……还是有一些数据是无效的。。。














